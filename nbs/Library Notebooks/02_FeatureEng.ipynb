{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782f57a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "> This module under Package PredictiveMaintenance2 defines functions for feature exploration,feature creation,feature selection,feature encoding and feature extraction.\n",
    "> feature transformation functions are imported from preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad188e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp FeatureEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c008238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570573fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9910a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def explain_features(dataset_df : pd.DataFrame, # Pandas DataFrame object of dataset\n",
    "                    machine_Unique_Identifer :str = None, # a unique ID to identify machine\n",
    "                    machine_features: list = None, # a list of machine features such as vendor_name, machine_type,manufacturer ....\n",
    "                    observation_date : str = None, # date when observation is recorded\n",
    "                    survival_time : int = None, # age of equipment till observation_date\n",
    "                    failure : int = None, # event of failute . In most cases failure = 0 means not failed yet, failure=1 means equipment is failed\n",
    "                    sensor_values:list = None # measured values of multiple sensors\n",
    "                    ):\n",
    "    try:\n",
    "        \n",
    "        # machine_unique_identifier\n",
    "        if machine_Unique_Identifer:\n",
    "            unique_device_types = pd.DataFrame(dataset_df.groupby([machine_Unique_Identifer]).agg(['count']))\n",
    "            print(f\"There are {(unique_device_types.shape)[0]} unique machines\\n\")\n",
    "            \n",
    "        \n",
    "        # observation_date\n",
    "        if observation_date:\n",
    "            unique_observation_dates = pd.DataFrame(dataset_df.groupby([observation_date]).agg(['count']))\n",
    "            print(f\"Observations are recorded for {(unique_observation_dates.shape)[0]} days\\n\")\n",
    "            print(f\" First 5 unique dates are \\n{(unique_observation_dates.index)[:5]}\\n\")\n",
    "            \n",
    "            \n",
    "        # machine features = meta_features\n",
    "        if machine_features:\n",
    "            # counter for each machine feature[category,count]\n",
    "            # visualize as categorical variable [Visualize module]\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # survival time\n",
    "        if survival_time:\n",
    "            # plot histogram of survival time with failure event [visualize module]\n",
    "            pass\n",
    "        \n",
    "        # failure\n",
    "        if failure:\n",
    "            # calculate failure rate\n",
    "            failure_count = dataset_df.groupby(['failure'])['device'].agg('count')\n",
    "            print(f\"Number of records where, \\nFailure = FALSE are {failure_count[0]} \\nFailure = TRUE are {failure_count[1]}\")\n",
    "            failure_rate = failure_count[1]*100/(dataset_df.shape)[0]\n",
    "            print(f\"\\nPercentage of failures : {failure_rate:.3f}%\")\n",
    "            \n",
    "            if failure_rate < 50:\n",
    "                print(f\"\\n--Warning---: \\nNumber of record of event type failure are too low \\nDataset is unbalanced.\\nUse expand_target_window function\")\n",
    "        \n",
    "        # sensor_values\n",
    "        if len(sensor_values):\n",
    "            for sensor in sensor_values:\n",
    "                # provide feature statistics - pd.describe()\n",
    "                pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    else:\n",
    "        #return unique_device_types\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e2c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but in this case they are not much useful because most sensor values do not change'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# function to create running summeries for sensor_values by feature_window = * days\n",
    "\"\"\"but in this case they are not much useful because most sensor values do not change\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def expand_target_window(dataset_df: pd.DataFrame,\n",
    "                         target_window: int,\n",
    "                         observation_date : str,\n",
    "                         machine_Unique_Identifer :str,\n",
    "                         rul:str = None,\n",
    "                         survival_time : str = None,\n",
    "                         failure_date : str = None\n",
    "                        ):\n",
    "    try:\n",
    "        # if rul column is not given in dataset- calculate it \n",
    "        if rul is None:\n",
    "            rul = calculate_rul(dataset_df,observation_date,)\n",
    "        else:\n",
    "            pass \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def remove_invalid_records(dataset_df : pd.DataFrame, # Pandas DataFrame object of dataset\n",
    "                    machine_Unique_Identifer :str = None, # a unique ID to identify machine\n",
    "                    observation_date : str = None, # date when observation is recorded\n",
    "                    failure : str = None, # event of failute . In most cases failure = 0 means not failed yet, failure=1 means equipment is failed\n",
    "                    ):\n",
    "    try :\n",
    "        # format observation_date field if it comes in as string\n",
    "        dataset_df[observation_date] = pd.to_datetime(dataset_df[observation_date],format = 'mixed')\n",
    "\n",
    "        # last date of entire experiement\n",
    "        last_date = dataset_df[observation_date].max()\n",
    "\n",
    "        # group by machine ID and find the last date of observation for each machine which is nothing but failure date\n",
    "        last_observation_dates = dataset_df.groupby(machine_Unique_Identifer)[observation_date].max().reset_index()\n",
    "        last_observation_dates = last_observation_dates.rename(columns = {observation_date :'last_observation_date'})\n",
    "\n",
    "        # merge last_observation_dates and dataset_df\n",
    "        dataset_df = pd.merge(dataset_df, last_observation_dates, on=machine_Unique_Identifer) \n",
    "        \n",
    "        last_failure_dates = dataset_df.groupby(machine_Unique_Identifer)[failure].max().reset_index()\n",
    "        last_failure_dates = last_failure_dates.rename(columns = {failure :'last_failure'})\n",
    "\n",
    "        # merge last_failure_dates and dataset_df\n",
    "        dataset_df = pd.merge(dataset_df, last_failure_dates, on=machine_Unique_Identifer) \n",
    "        \n",
    "        valid_mask = ( \n",
    "                        (dataset_df['last_observation_date'] == last_date)|\n",
    "                        (dataset_df['last_failure']==1))\n",
    "\n",
    "        # Filter out invalid records\n",
    "        dataset_df = dataset_df[valid_mask].sort_values([machine_Unique_Identifer,observation_date]).reset_index(drop=True)\n",
    "        \n",
    "        # remove intermedite results\n",
    "        dataset_df.drop(columns=['last_observation_date','last_failure'],axis=0,inplace=True)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return dataset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced90d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calculate_rul(dataset_df :pd.DataFrame,\n",
    "                  observation_date : str,\n",
    "                  machine_Unique_Identifer :str\n",
    "                 ):\n",
    "    try:\n",
    "        # format observation_date field if it comes in as string\n",
    "        dataset_df[observation_date] = pd.to_datetime(dataset_df[observation_date],format = 'mixed')\n",
    "\n",
    "        # group by machine ID and find the last date of observation for each machine which is nothing but failure date\n",
    "        last_observation_dates = dataset_df.groupby(machine_Unique_Identifer)[observation_date].max().reset_index()\n",
    "        last_observation_dates = last_observation_dates.rename(columns = {observation_date :'last_observation_date'})\n",
    "\n",
    "        # merge last_observation_dates and dataset_df\n",
    "        dataset_df = pd.merge(dataset_df, last_observation_dates, on=machine_Unique_Identifer)\n",
    "                              \n",
    "        # calculate the time difference between the last observation date and each observation date for that machine\n",
    "        time_diff = dataset_df['last_observation_date'] - dataset_df[observation_date]\n",
    "        \n",
    "        # calculate the RUL for each observation\n",
    "        dataset_df['RUL'] = (time_diff.dt.days).astype(int)\n",
    "        dataset_df.sort_values(by=observation_date,inplace=True)\n",
    "        \n",
    "        # view modified dataset after adding RUL and dropping intermediate columns created\n",
    "        dataset_df.reset_index(inplace=True)\n",
    "\n",
    "        # drop intermediate columns function\n",
    "        dataset_df.drop(columns=['index','last_observation_date'],axis=0,inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797afddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def to_numerical(dataset_df :pd.DataFrame):\n",
    "    try:\n",
    "        column_types = dataset_df.dtypes\n",
    "        for column in dataset_df.columns:\n",
    "            if column_types[column] == object:\n",
    "                if column == 'date':\n",
    "                    pass\n",
    "                else:\n",
    "                    dataset_df[column] = pd.factorize(dataset_df[column])[0]\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    else:\n",
    "        return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calculate_survival_time(dataset_df :pd.DataFrame,\n",
    "                  observation_date : str,\n",
    "                  machine_Unique_Identifer :str\n",
    "                 ):\n",
    "    try:\n",
    "        # format observation_date field if it comes in as string\n",
    "        dataset_df[observation_date] = pd.to_datetime(dataset_df[observation_date],format = 'mixed')\n",
    "        \n",
    "        # group by machine ID and find the first date of observation for each machine\n",
    "        \n",
    "        # if last_date_of_observation the machine is not failed then it's survival time changes\n",
    "        \n",
    "        # group by machine ID and find the first date of observation for each machine \n",
    "        first_observation_dates = dataset_df.groupby(machine_Unique_Identifer)[observation_date].min().reset_index()\n",
    "        first_observation_dates = first_observation_dates.rename(columns = {observation_date :'first_observation_date'})\n",
    "        \n",
    "        # merge first_observation_dates and dataset_df\n",
    "        dataset_df_rul = pd.merge(dataset_df, first_observation_dates, on=machine_Unique_Identifer) \n",
    "        \n",
    "        # calculate the time difference between the first observation date and each observation date for that machine to find it's age\n",
    "        time_diff = dataset_df_rul[observation_date] - dataset_df_rul['first_observation_date']\n",
    "        \n",
    "        # calculate the survival time for each observation\n",
    "        dataset_df_rul['SurvivalTime'] = ((time_diff.dt.days).astype(int))+1\n",
    "        dataset_df_rul.sort_values(by=observation_date,inplace=True)\n",
    "        \n",
    "        # view modified dataset after adding survival time and dropping intermediate columns created\n",
    "        dataset_df_rul.reset_index(inplace=True)\n",
    "\n",
    "        # drop intermediate columns function\n",
    "        dataset_df_rul.drop(columns=['index','first_observation_date'],axis=0,inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return dataset_df_rul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512de582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7b431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4bcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
