[
  {
    "objectID": "objective1_predict_rul-copy1.html",
    "href": "objective1_predict_rul-copy1.html",
    "title": "Objective 1 : Predict RUL -O",
    "section": "",
    "text": "Predictive Maintenance (PdM) is a great application of Survival Analysis since it consists in predicting when equipment failure will occur and therefore alerting the maintenance team to prevent that failure.\n\nObjectives\n\n\nTo estimate Remaining Useful Time(RUL) of a machine/component\n\n\n\n\nload dataset\n\n# load pre processed dataset\nmachineData = pd.read_csv('Machine_Data_Preprocessed.csv')\nmachineData.head(2)\n\n\n\n\n\n\n\n\ndate\ndevice\nfailure\nmetric1\nmetric2\nmetric3\nmetric4\nmetric5\nmetric6\nmetric7\nmetric8\nmetric9\nRUL\nSurvivalTime\n\n\n\n\n0\n2015-01-01\n0\n0\n141503600\n0\n0\n1\n19\n494462\n16\n16\n3\n18\n1\n\n\n1\n2015-01-01\n1\n0\n55587136\n0\n0\n0\n7\n199132\n0\n0\n0\n214\n1\n\n\n\n\n\n\n\n\n# convert to datetime \nmachineData['date'] = pd.to_datetime(machineData['date'])\n\n\n# sort values by device and date\nmachineData = machineData.sort_values(['device','date'],ascending= True).reset_index(drop=True)\nmachineData.head(2)\n\n\n\n\n\n\n\n\ndate\ndevice\nfailure\nmetric1\nmetric2\nmetric3\nmetric4\nmetric5\nmetric6\nmetric7\nmetric8\nmetric9\nRUL\nSurvivalTime\n\n\n\n\n0\n2015-01-01\n0\n0\n141503600\n0\n0\n1\n19\n494462\n16\n16\n3\n18\n1\n\n\n1\n2015-01-02\n0\n0\n161679800\n0\n0\n1\n19\n495730\n16\n16\n3\n17\n2\n\n\n\n\n\n\n\n\nunique_device_types = pd.DataFrame(machineData.groupby(['device']).agg(['count']))\nunique_device_types.shape\n\n(137, 13)\n\n\nthere are 137 devices - out which 106 devices have failed during observation and 31 devices have lasted till the end of observation.\n\n\nModeling - without Feature selection\n\nsplit the data\n\n# fit entire dataset when working with feature selection\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\ny = machineData['RUL']\n\n# split the data\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n\n\nfit the model\n\n# Create a Random Forest Regressor model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model to the training data\nrf_model.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\npredictions\n\n# Make predictions on the testing data\ny_pred = rf_model.predict(x_test)\n\n\n\nvalidations\n\n# Calculate the Mean Squared Error (MSE) and R-squared value of the predictions\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\n# Print the results\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"R-squared value: {r2}\")\n\nMean Squared Error: 105.06250148795198\nMean Absolute Error: 4.667849196330892\nR-squared value: 0.9842798735885625\n\n\n\n\n\nModeling with feature selection - randomforest regressor\n\nBoruta\n\n# fit entire dataset when working with feature selection\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\ny = machineData['RUL']\n\n# convert to numpy to apply boruta\nx_arr = x.to_numpy()\ny_arr = y.to_numpy()\n\n\n# Instantiate the random forest regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\n\n\n# Instantiate the Boruta object\nboruta = BorutaPy(estimator=rf, n_estimators='auto', verbose=2, alpha=0.05, max_iter=10, random_state=42)\n\n# Fit the Boruta object to the training data\nboruta.fit(x_arr, y_arr)\n\nIteration:  1 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  2 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  3 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  4 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  5 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  6 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  7 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  8 / 10\nConfirmed:  9\nTentative:  1\nRejected:   0\nIteration:  9 / 10\nConfirmed:  9\nTentative:  1\nRejected:   0\n\n\nBorutaPy finished running.\n\nIteration:  10 / 10\nConfirmed:  9\nTentative:  0\nRejected:   0\n\n\nBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x2250A8F1E40),\n         max_iter=10, n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x2250A8F1E40, verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BorutaPyBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x2250A8F1E40),\n         max_iter=10, n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x2250A8F1E40, verbose=2)estimator: RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x2250A8F1E40)RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x2250A8F1E40)\n\n\n\n# call transform() on x to filter it down to selected features\nx_selected = boruta.transform(x_arr)\n\n\n\nsplit the data\n\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\ny = machineData['RUL']\n\n# split the data\nx_train, x_test, y_train, y_test = train_test_split(x_selected,y, test_size=0.2, random_state=42)\n\n\n\nfit the model\n\n# Create a Random Forest Regressor model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model to the training data\nrf_model.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nPredictions\n\n# Make predictions on the testing data\ny_pred = rf_model.predict(x_test)\n\n\n\nValidations\n\n# Calculate the Mean Squared Error (MSE) and R-squared value of the predictions\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\n# Print the results\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"R-squared value: {r2}\")\n\nMean Squared Error: 83.94598232058489\nMean Absolute Error: 4.123105926662051\nR-squared value: 0.9874394628423803\n\n\n\n\ninterpret the model\n\n# shap to feature explanation\n\n# Fit the explainer to sample x_test/ full dataset x_selected\nexplainer = shap.Explainer(rf_model.predict, x_selected)\n\n# Calculates the SHAP values - It takes some time\nshap_values = explainer(x_selected)\nshap_values\n\nExact explainer: 19806it [3:41:26,  1.49it/s]                                                                          \n\n\n.values =\narray([[ 3.73907837e+00,  1.32506122e+00, -1.17333856e-01, ...,\n        -2.73252130e+00, -2.40410608e+00, -7.34367051e+00],\n       [ 3.73907837e+00,  1.32506122e+00, -1.17333856e-01, ...,\n        -2.73252130e+00, -2.40410608e+00, -7.34367051e+00],\n       [ 3.70568670e+00,  1.32506122e+00, -1.17543856e-01, ...,\n        -2.73768463e+00, -2.42076108e+00, -7.36847884e+00],\n       ...,\n       [-7.33903858e+01,  1.89993439e+00, -1.93152320e-02, ...,\n         2.65801312e-01,  2.58013244e-01, -4.20636435e-01],\n       [-7.33903858e+01,  1.89993439e+00, -1.93152320e-02, ...,\n         2.65801312e-01,  2.58013244e-01, -4.20636435e-01],\n       [-7.33903858e+01,  1.89993439e+00, -1.93152320e-02, ...,\n         2.65801312e-01,  2.58013244e-01, -4.20636435e-01]])\n\n.base_values =\narray([115.95513874, 115.95513874, 115.95513874, ..., 115.95513874,\n       115.95513874, 115.95513874])\n\n.data =\narray([[  0,   0,   0, ...,  16,  16,   3],\n       [  0,   0,   0, ...,  16,  16,   3],\n       [  0,   0,   0, ...,  16,  16,   3],\n       ...,\n       [136,   0,   0, ...,   0,   0,   0],\n       [136,   0,   0, ...,   0,   0,   0],\n       [136,   0,   0, ...,   0,   0,   0]], dtype=int64)\n\n\n\n# bee swarm plot to understand feature importance on dataset\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\nModel - Pycaret\nModel selection, find best model for the task\n\n# without any customization\nreg_normal_setup = setup(data=machineData,\n                        target = 'RUL',\n                        ignore_features=['date','failure','SurvivalTime'],\n                        )\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n2105\n\n\n1\nTarget\nRUL\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(19805, 14)\n\n\n4\nTransformed data shape\n(19805, 11)\n\n\n5\nTransformed train set shape\n(13863, 11)\n\n\n6\nTransformed test set shape\n(5942, 11)\n\n\n7\nIgnore features\n3\n\n\n8\nNumeric features\n10\n\n\n9\nPreprocess\nTrue\n\n\n10\nImputation type\nsimple\n\n\n11\nNumeric imputation\nmean\n\n\n12\nCategorical imputation\nmode\n\n\n13\nFold Generator\nKFold\n\n\n14\nFold Number\n10\n\n\n15\nCPU Jobs\n-1\n\n\n16\nUse GPU\nFalse\n\n\n17\nLog Experiment\nFalse\n\n\n18\nExperiment Name\nreg-default-name\n\n\n19\nUSI\ncbca\n\n\n\n\n\n\n# best_model = compare_models(sort='MAE')\n# TT - training time (sec) , MAPE = mean absolute percentage error\nbest_model = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\net\nExtra Trees Regressor\n4.7886\n110.9730\n10.4697\n0.9837\n0.2905\n0.1562\n1.1670\n\n\nrf\nRandom Forest Regressor\n5.1713\n119.2823\n10.8855\n0.9825\n0.3154\n0.1974\n1.7620\n\n\nxgboost\nExtreme Gradient Boosting\n7.7887\n145.2248\n12.0320\n0.9787\n0.3780\n0.2938\n0.8490\n\n\ndt\nDecision Tree Regressor\n5.1639\n206.3064\n14.2304\n0.9699\n0.3247\n0.1724\n0.7380\n\n\ncatboost\nCatBoost Regressor\n11.4367\n239.6256\n15.4688\n0.9649\n0.4592\n0.4481\n2.4680\n\n\nlightgbm\nLight Gradient Boosting Machine\n13.1843\n333.5461\n18.2586\n0.9511\n0.5210\n0.5564\n0.8650\n\n\ngbr\nGradient Boosting Regressor\n32.5724\n1830.9103\n42.7822\n0.7319\n0.7730\n1.2569\n0.8610\n\n\nada\nAdaBoost Regressor\n52.9810\n3993.9509\n63.1892\n0.4152\n1.0228\n2.2936\n0.6190\n\n\nen\nElastic Net\n64.7947\n6149.9207\n78.4092\n0.0997\n1.1160\n2.7456\n0.6330\n\n\nbr\nBayesian Ridge\n64.7965\n6149.7436\n78.4081\n0.0997\n1.1159\n2.7465\n1.0890\n\n\nlasso\nLasso Regression\n64.7954\n6149.8087\n78.4085\n0.0997\n1.1160\n2.7456\n0.5470\n\n\nlr\nLinear Regression\n64.7929\n6150.2283\n78.4111\n0.0996\n1.1159\n2.7452\n1.3520\n\n\nridge\nRidge Regression\n64.7929\n6150.2283\n78.4111\n0.0996\n1.1159\n2.7452\n0.5640\n\n\nlar\nLeast Angle Regression\n64.8109\n6154.2442\n78.4366\n0.0991\n1.1163\n2.7491\n0.5870\n\n\nllar\nLasso Least Angle Regression\n64.8134\n6153.7989\n78.4339\n0.0991\n1.1164\n2.7494\n0.7530\n\n\nomp\nOrthogonal Matching Pursuit\n69.2270\n6836.4367\n82.6708\n-0.0007\n1.1751\n3.0754\n0.7100\n\n\ndummy\nDummy Regressor\n69.2278\n6835.9223\n82.6677\n-0.0007\n1.1751\n3.0754\n0.6230\n\n\nknn\nK Neighbors Regressor\n68.3816\n7054.1118\n83.9720\n-0.0329\n1.1642\n3.0449\n0.9670\n\n\nhuber\nHuber Regressor\n74.6127\n8710.6917\n93.3204\n-0.2754\n1.1988\n2.5091\n1.0940\n\n\npar\nPassive Aggressive Regressor\n752.8186\n4299846.0654\n882.8474\n-657.4914\n1.9168\n23.1708\n1.0730\n\n\n\n\n\n\n\n\n\nprint(best_model)\n\nExtraTreesRegressor(n_jobs=-1, random_state=2329)\n\n\n\nHyperparameter tuning\nFind best combination of hyperparameters from Hyperparameter space. - Every combination in HP space is model - To search effective combination of HP - two approaches - GridSeachCV and RandomSeachCV where CV = cross-validation - with idea of hyperparameter tuning, split the data into trainig, testing and validation to prevent ‘Data Leakage’\n\n# tune the hyper parameters of the best model - case when : tree model\n\"\"\"\nimportant : max_features, min_samples_leaf,n_estimators\n\"\"\"\nparams = {\n    'n_estimators': [30],           #number of trees\n    'criterion': ['absolute_error'], #['squared_error','absolute_error','friedman_mse','poisson']\n    'max_depth': [3],             # if none nodes are expanded until all leaves are pure\n    'min_samples_split': [2],        # min samples required to split internal node [int or float or default=2]\n    'min_samples_leaf' : [1],        # a split is considered if both branches has min_samples_leaf [int or float or default=1]\n    # min_weight_fraction_leaf : 0.0\n    'max_features':[1.0],            # max features to consider at each split [int or float or none or 'sqrt' or 'log'] default = 1.0 =&gt; max_features=n_features\n    'max_leaf_nodes': [None],        # int = first best nodes or infinite nodes if none\n    'min_impurity_decrease' : [0.0], # node split is considered if decrease of impurity due to this split &lt; min_impurity_decrease\n    'bootstrap':[False],             # True : use bootstrap samples to build the tree , False: Use whole dataset to build each tree\n    'oob_score' : [False],           # out of bag samples to estimate generalization score , only when bootstrap= True\n    # 'n_jobs'=1,                    # number of jobs to run in parallel\n    'random_state' : [None],         # for bootstrapping, draw of split of max_Features\n    'verbose' : [True], \n    # 'warm_start' : False           # True : reuse the solution of previous call to add estimator to ensemble\n    'ccp_alpha' : [0.0],             # minimal cost complexity pruning ,0.0 = no pruning , largest cost complexity &lt; ccp_alpha is choosen\n    'max_samples' : [None]           #max samples to draw to train estimator (if bootstrapping = True)   \n}\n\n\nextra_tree_tuned_model = tune_model(best_model,\n                         fold = 2,\n                         n_iter=10,\n                         custom_grid = params,\n                         optimize='MAE',\n                         search_library='scikit-learn',\n                         search_algorithm='random',\n                         choose_better = True, # True : return only better performing model b/w original estimator or tuned model\n                         return_tuner= False, # if True: returns tuner object , False: returns estimator object very important difference\n                         return_train_score=True\n                        )\n\n\n\n\n\n\n\n\n\n \n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nSplit\nFold\n \n \n \n \n \n \n\n\n\n\nCV-Train\n0\n59.3845\n5405.1934\n73.5200\n0.2121\n1.0417\n2.3260\n\n\n1\n58.3216\n5124.0370\n71.5824\n0.2476\n1.0526\n2.3135\n\n\nCV-Val\n0\n58.7267\n5332.1671\n73.0217\n0.2170\n1.0518\n2.2680\n\n\n1\n59.2478\n5226.3246\n72.2933\n0.2381\n1.0447\n2.3805\n\n\nCV-Train\nMean\n58.8531\n5264.6152\n72.5512\n0.2298\n1.0471\n2.3198\n\n\nStd\n0.5315\n140.5782\n0.9688\n0.0178\n0.0054\n0.0063\n\n\nCV-Val\nMean\n58.9872\n5279.2459\n72.6575\n0.2276\n1.0483\n2.3242\n\n\nStd\n0.2605\n52.9212\n0.3642\n0.0106\n0.0036\n0.0562\n\n\nTrain\nnan\n57.8880\n5051.5973\n71.0746\n0.2609\n1.0421\n2.3370\n\n\n\n\n\n\n\n\nFitting 2 folds for each of 1 candidates, totalling 2 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\nprint(extra_tree_tuned_model)\n\nExtraTreesRegressor(n_jobs=-1, random_state=937)\n\n\n\n# avaiable plots in pycaret\navailable_plots = [\n    'pipeline', # schematic drawing of the preprocessing pipeline\n    'residuals_interactive', # interactive residual plots\n    'residuals', \n    'error',\n    'cooks', # cooks distance plot ?\n    'rfe',\n    'learning',\n    'vc',\n    'manifold',\n    'feature',\n    'feature_all',\n    'parameter',\n    'tree' # Decision tree\n]\n\n\nplot_model(extra_tree_tuned_model,plot='error')\n\n\n\n\n\n\n\n\nplot_model(best_model,plot=\"error\")\n\n\nplot_model(extra_tree_tuned_model,plot='feature')\n\n\n\n\n\n\n\n\n# Interpret the model - dependent on SHAP\n# 40 minutes\ninterpret_model(extra_tree_tuned_model,plot='reason',observation=32)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# save model\n# save_model(best_model,'Extra_Trees_regressor_model')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\Lalitha\\AppData\\Local\\Temp\\joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(include=['device', 'metric1', 'metric2',\n                                              'metric3', 'metric4', 'metric5',\n                                              'metric6', 'metric7', 'metric8',\n                                              'metric9'],\n                                     transformer=SimpleImputer())),\n                 ('categorical_imputer',\n                  TransformerWrapper(include=[],\n                                     transformer=SimpleImputer(strategy='most_frequent'))),\n                 ('trained_model',\n                  ExtraTreesRegressor(n_jobs=-1, random_state=937))]),\n 'Extra_Trees_regressor_model.pkl')\n\n\n\n#dashboard(model) - uses explainerDashboard library \n# best_model = load_model(\"Extra_Trees_regressor_model\")\ndashboard_kwargs = {\n    'shap_interaction':False\n}\ndashboard(best_model,display_format=\"inline\",dashboard_kwargs=dashboards)\n\nTransformation Pipeline and Model Successfully Loaded\nWARNING: Parameter shap='guess', but failed to guess the type of shap explainer to use for Pipeline. Defaulting to the model agnostic shap.KernelExplainer (shap='kernel'). However this will be slow, so if your model is compatible with e.g. shap.TreeExplainer or shap.LinearExplainer then pass shap='tree' or shap='linear'!\nWARNING: For shap='kernel', shap interaction values can unfortunately not be calculated!\nWarning: shap values for shap.KernelExplainer get calculated against X_background, but paramater X_background=None, so using shap.sample(X, 50) instead\nGenerating self.shap_explainer = shap.KernelExplainer(model, X)...\nBuilding ExplainerDashboard..\nWARNING: the number of idxs (=5942) &gt; max_idxs_in_dropdown(=1000). However with your installed version of dash(2.9.3) dropdown search may not work smoothly. You can downgrade to `pip install dash==2.6.2` which should work better for now...\nThe explainer object has no decision_trees property. so setting decision_trees=False...\nGenerating layout...\nCalculating shap values...\nCalculating predictions...\nCalculating residuals...\nCalculating absolute residuals...\nWarning: mean-absolute-percentage-error is very large (80954136014716.06), you can hide it from the metrics by passing parameter show_metrics...\nWarning: mean-absolute-percentage-error is very large (80954136014716.06), you can hide it from the metrics by passing parameter show_metrics...\nWarning: mean-absolute-percentage-error is very large (80954136014716.06), you can hide it from the metrics by passing parameter show_metrics...\nCalculating dependencies...\nCalculating importances...\nReminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')\nRegistering callbacks...\nStarting ExplainerDashboard inline (terminate it with ExplainerDashboard.terminate(8050))\nDash is running on http://127.0.0.1:8050/"
  },
  {
    "objectID": "understand_dataset.html",
    "href": "understand_dataset.html",
    "title": "Understanding Dataset",
    "section": "",
    "text": "Predictive Maintenance (PdM) is a great application of Survival Analysis since it consists in predicting when equipment failure will occur and therefore alerting the maintenance team to prevent that failure."
  },
  {
    "objectID": "understand_dataset.html#encoding-features--categorical-to-numerical",
    "href": "understand_dataset.html#encoding-features--categorical-to-numerical",
    "title": "Understanding Dataset",
    "section": "Encoding features- categorical to numerical",
    "text": "Encoding features- categorical to numerical\n\n# encoding categorical variables to numerical\nmachine_data_rul = FeatureEng.to_numerical(dataset_df = machine_data_rul)\nmachine_data_rul.head()\n\n\n\n\n\n\n\n\ndate\ndevice\nfailure\nmetric1\nmetric2\nmetric3\nmetric4\nmetric5\nmetric6\nmetric7\nmetric8\nmetric9\nRUL\nSurvivalTime\n\n\n\n\n0\n2015-01-01\n0\n0\n141503600\n0\n0\n1\n19\n494462\n16\n16\n3\n18\n1\n\n\n1\n2015-01-01\n1\n0\n55587136\n0\n0\n0\n7\n199132\n0\n0\n0\n214\n1\n\n\n2\n2015-01-01\n2\n0\n12568128\n136\n0\n2\n6\n380112\n0\n0\n2\n198\n1\n\n\n3\n2015-01-01\n3\n0\n50147888\n528\n0\n4\n9\n381198\n32\n32\n3\n6\n1\n\n\n4\n2015-01-01\n4\n0\n8471680\n0\n0\n0\n11\n436682\n0\n0\n0\n44\n1\n\n\n\n\n\n\n\n\n# apply smote ---\n\n\n# save the data to a .csv file\nmachine_data_rul.to_csv('Machine_Data_Preprocessed.csv',index = False)"
  },
  {
    "objectID": "objective2_probability of failure in next n days.html",
    "href": "objective2_probability of failure in next n days.html",
    "title": "Objective 2 : Predict Probability of Failure in Next N days",
    "section": "",
    "text": "Predictive Maintenance (PdM) is a great application of Survival Analysis since it consists in predicting when equipment failure will occur and therefore alerting the maintenance team to prevent that failure.\n\nObjectives\n\n\nTo Predict Probability of Failure in Next N days\n\n\n\n\nFeature selection\n\nload dataset\n\n# load pre processed dataset\nmachineData = pd.read_csv('Machine_Data_Preprocessed.csv')\nmachineData.head(2)\n\n\n\n\n\n\n\n\ndate\ndevice\nfailure\nmetric1\nmetric2\nmetric3\nmetric4\nmetric5\nmetric6\nmetric7\nmetric8\nmetric9\nRUL\nSurvivalTime\n\n\n\n\n0\n2015-01-01\n0\n0\n141503600\n0\n0\n1\n19\n494462\n16\n16\n3\n18\n1\n\n\n1\n2015-01-01\n1\n0\n55587136\n0\n0\n0\n7\n199132\n0\n0\n0\n214\n1\n\n\n\n\n\n\n\n\n# sort values by device and date\nmachineData = machineData.sort_values(['device','date'],ascending= True).reset_index(drop=True)\n\n# get last record of each device\nmachineData['date'] = pd.to_datetime(machineData['date'])\nlast_date = machineData.groupby('device')['date'].transform(max) == machineData['date']\n\n\n# Filter the data to keep only the rows with the maximum observation date for each device\nmachineData = machineData[last_date].reset_index(drop=True)\n\n# Print the final output\nprint(machineData.head(2))\n\n        date  device  failure    metric1  metric2  metric3  metric4  metric5  \\\n0 2015-01-19       0        1   64499464        0        0        1       19   \n1 2015-08-03       1        1  110199904      240        0        0        8   \n\n   metric6  metric7  metric8  metric9  RUL  SurvivalTime  \n0   514661       16       16        3    0            19  \n1   294852        0        0        0    0           215  \n\n\n\n\nsplit the data\n\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\n\n# Create a structured array for the survival analysis\ny = np.zeros(len(machineData), dtype=[('failure', 'bool'), ('SurvivalTime', 'float')])\ny['failure'] = machineData['failure'].astype(bool)\ny['SurvivalTime'] = machineData['SurvivalTime'].astype(float)\n\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.15,random_state=42)\n\n\n# Normalize the data using quantile normalization\nqn = QuantileTransformer(output_distribution='normal', random_state=42)\nx_train_norm = qn.fit_transform(x_train)\nx_test_norm = qn.transform(x_test)\n\nn_quantiles (1000) is greater than the total number of samples (116). n_quantiles is set to n_samples.\n\n\n\n\nSelectFromModel\n\n\n\nModeling - CoxPH Survival Model\n\n# Create a instance Cox PH survival model\nestimator = CoxPHSurvivalAnalysis(alpha=0.05)\n\n# Fit the model to the training data\nestimator.fit(x_train_norm, y_train)\n\nCoxPHSurvivalAnalysis(alpha=0.05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CoxPHSurvivalAnalysisCoxPHSurvivalAnalysis(alpha=0.05)\n\n\n\n\nPredictions\n\n# Predict the probability of failure in the next n days for each machine in the testing set\ny_pred = estimator.predict(x_test_norm)\n\n\n# Predict the survival functions for the testing set\nsurvival_functions = estimator.predict_survival_function(x_test_norm)\n\n\nprint(len(survival_functions))\n\n21\n\n\nCalculate the probability of failure in the next n days using the predicted hazard ratios and the survival function\n\n# Get the probability of failure in the next n days\nn_days = 20 \nfailure_prob = []\n\nfor sf in survival_functions:\n    t_idx = np.argmin(np.abs(sf.x - n_days))\n    failure_prob.append(1 - sf.y[t_idx])\n    \nfailure_prob = np.array(failure_prob)\ncolname = 'Next '+str(n_days)+' Days'\ndf = pd.DataFrame(failure_prob,columns=[colname])\nprint(df)\n\n# Print the prObability of failure for each device in the testing set\nfor i, prob in enumerate(failure_prob):\n    if prob&gt;0.5:\n        print(f\"----Device-{i} : {prob*100:.2f}-----\")\n    else:\n        print(f\"Device-{i} : {prob*100:.2f}\")\n\n    Next 20 Days\n0       0.190475\n1       0.314233\n2       0.029818\n3       0.042481\n4       0.175501\n5       0.035182\n6       0.139161\n7       0.079668\n8       0.390220\n9       0.185896\n10      0.535268\n11      0.052561\n12      0.087314\n13      0.050598\n14      0.040139\n15      0.094391\n16      0.182914\n17      0.445223\n18      0.082525\n19      0.053227\n20      0.073477\nDevice-0 : 19.05\nDevice-1 : 31.42\nDevice-2 : 2.98\nDevice-3 : 4.25\nDevice-4 : 17.55\nDevice-5 : 3.52\nDevice-6 : 13.92\nDevice-7 : 7.97\nDevice-8 : 39.02\nDevice-9 : 18.59\n----Device-10 : 53.53-----\nDevice-11 : 5.26\nDevice-12 : 8.73\nDevice-13 : 5.06\nDevice-14 : 4.01\nDevice-15 : 9.44\nDevice-16 : 18.29\nDevice-17 : 44.52\nDevice-18 : 8.25\nDevice-19 : 5.32\nDevice-20 : 7.35\n\n\n\n# get the probability of data for every 10 days in 2 months\nfinal_dict = {}\nfor n_days in range(10,60,10):\n    failure_prob = []\n\n    for sf in survival_functions:\n        t_idx = np.argmin(np.abs(sf.x - n_days))\n        t_idx = 1 - sf.y[t_idx]\n        failure_prob.append(round(t_idx*100,2))\n\n    failure_prob = np.array(failure_prob)\n    colname = 'Next '+str(n_days)+' Days (%)'\n    final_dict[colname] = failure_prob\n\nfinal_df = pd.DataFrame(final_dict)\n\nprint(final_df)\n\n    Next 10 Days (%)  Next 20 Days (%)  Next 30 Days (%)  Next 40 Days (%)  \\\n0               3.56             19.05             27.79             35.30   \n1               6.26             31.42             44.08             54.03   \n2               0.52              2.98              4.56              6.05   \n3               0.74              4.25              6.47              8.56   \n4               3.25             17.55             25.72             32.81   \n5               0.61              3.52              5.37              7.11   \n6               2.54             13.92             20.62             26.56   \n7               1.41              7.97             12.01             15.72   \n8               8.13             39.02             53.34             63.91   \n9               3.46             18.59             27.16             34.54   \n10             12.31             53.53             69.30             79.38   \n11              0.92              5.26              7.98             10.53   \n12              1.55              8.73             13.13             17.16   \n13              0.89              5.06              7.69             10.15   \n14              0.70              4.01              6.12              8.09   \n15              1.69              9.44             14.17             18.48   \n16              3.40             18.29             26.75             34.05   \n17              9.61             44.52             59.67             70.30   \n18              1.47              8.25             12.43             16.26   \n19              0.93              5.32              8.08             10.66   \n20              1.30              7.35             11.10             14.55   \n\n    Next 50 Days (%)  \n0              37.78  \n1              57.13  \n2               6.57  \n3               9.29  \n4              35.17  \n5               7.73  \n6              28.57  \n7              17.01  \n8              67.07  \n9              36.99  \n10             82.11  \n11             11.42  \n12             18.55  \n13             11.01  \n14              8.79  \n15             19.96  \n16             36.47  \n17             73.37  \n18             17.59  \n19             11.56  \n20             15.75  \n\n\n\n# get the probability of data for every 10 days in 2 months\ndef prob_of_failure(survival_functions,\n                   iteration_time:list = ['D',10], #repeat predictions for every [D= date,M= month,Y=year ,,,Quantity of dates.]\n                   time_period:list = ['M',2] # make predictions in total time of D=Days,M=months,Y=Years\n                   ):\n    #M = 30D\n    #Y = 12M\n    iteration_predictions = []\n    minr = min(iteration_time[0],time_period[0])\n    \n    pass\n\n\n\nValidations\n\n# Evaluate the performance of the model on \nc_index = concordance_index_censored(y_test['failure'], y_test['SurvivalTime'], y_pred)\nprint(f\"C-index: {c_index[0]}\")\n\nC-index: 0.5738636363636364\n\n\n\n\nModeling - kaplan-Meier survival function from lifelines"
  },
  {
    "objectID": "objective1_predict_rul.html",
    "href": "objective1_predict_rul.html",
    "title": "Objective 1 : Predict RUL",
    "section": "",
    "text": "Predictive Maintenance (PdM) is a great application of Survival Analysis since it consists in predicting when equipment failure will occur and therefore alerting the maintenance team to prevent that failure.\n\nObjectives\n\n\nTo estimate Remaining Useful Time(RUL) of a machine/component\n\n\n\n\nload dataset\n\n# load pre processed dataset\nmachineData = pd.read_csv('Machine_Data_Preprocessed.csv')\nmachineData.head(2)\n\n\n\n\n\n\n\n\ndate\ndevice\nfailure\nmetric1\nmetric2\nmetric3\nmetric4\nmetric5\nmetric6\nmetric7\nmetric8\nmetric9\nRUL\nSurvivalTime\n\n\n\n\n0\n2015-01-01\n0\n0\n141503600\n0\n0\n1\n19\n494462\n16\n16\n3\n18\n1\n\n\n1\n2015-01-01\n1\n0\n55587136\n0\n0\n0\n7\n199132\n0\n0\n0\n214\n1\n\n\n\n\n\n\n\n\n# convert to datetime \nmachineData['date'] = pd.to_datetime(machineData['date'])\n\n\n# sort values by device and date\nmachineData = machineData.sort_values(['device','date'],ascending= True).reset_index(drop=True)\nmachineData.head(2)\n\n\n\n\n\n\n\n\ndate\ndevice\nfailure\nmetric1\nmetric2\nmetric3\nmetric4\nmetric5\nmetric6\nmetric7\nmetric8\nmetric9\nRUL\nSurvivalTime\n\n\n\n\n0\n2015-01-01\n0\n0\n141503600\n0\n0\n1\n19\n494462\n16\n16\n3\n18\n1\n\n\n1\n2015-01-02\n0\n0\n161679800\n0\n0\n1\n19\n495730\n16\n16\n3\n17\n2\n\n\n\n\n\n\n\n\nunique_device_types = pd.DataFrame(machineData.groupby(['device']).agg(['count']))\nunique_device_types.shape\n\n(137, 13)\n\n\nthere are 137 devices - out which 106 devices have failed during observation and 31 devices have lasted till the end of observation.\n\n\nModeling - without Feature selection\n\nsplit the data\n\n# fit entire dataset when working with feature selection\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\ny = machineData['RUL']\n\n# split the data\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n\n\nfit the model\n\n# Create a Random Forest Regressor model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model to the training data\nrf_model.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\npredictions\n\n# Make predictions on the testing data\ny_pred = rf_model.predict(x_test)\n\n\n\nvalidations\n\n# Calculate the Mean Squared Error (MSE) and R-squared value of the predictions\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\n# Print the results\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"R-squared value: {r2}\")\n\nMean Squared Error: 105.06250148795198\nMean Absolute Error: 4.667849196330892\nR-squared value: 0.9842798735885625\n\n\n\n\n\nModeling with feature selection - randomforest regressor\n\nBoruta\n\n# fit entire dataset when working with feature selection\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\ny = machineData['RUL']\n\n# convert to numpy to apply boruta\nx_arr = x.to_numpy()\ny_arr = y.to_numpy()\n\n\n# Instantiate the random forest regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\n\n\n# Instantiate the Boruta object\nboruta = BorutaPy(estimator=rf, n_estimators='auto', verbose=2, alpha=0.05, max_iter=10, random_state=42)\n\n# Fit the Boruta object to the training data\nboruta.fit(x_arr, y_arr)\n\nIteration:  1 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  2 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  3 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  4 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  5 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  6 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  7 / 10\nConfirmed:  0\nTentative:  10\nRejected:   0\nIteration:  8 / 10\nConfirmed:  9\nTentative:  1\nRejected:   0\nIteration:  9 / 10\nConfirmed:  9\nTentative:  1\nRejected:   0\n\n\nBorutaPy finished running.\n\nIteration:  10 / 10\nConfirmed:  9\nTentative:  0\nRejected:   0\n\n\nBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x2250A8F1E40),\n         max_iter=10, n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x2250A8F1E40, verbose=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BorutaPyBorutaPy(estimator=RandomForestRegressor(n_estimators=44,\n                                         random_state=RandomState(MT19937) at 0x2250A8F1E40),\n         max_iter=10, n_estimators='auto',\n         random_state=RandomState(MT19937) at 0x2250A8F1E40, verbose=2)estimator: RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x2250A8F1E40)RandomForestRegressorRandomForestRegressor(n_estimators=44,\n                      random_state=RandomState(MT19937) at 0x2250A8F1E40)\n\n\n\n# call transform() on x to filter it down to selected features\nx_selected = boruta.transform(x_arr)\n\n\n\nsplit the data\n\nx = machineData.drop(['date','failure','RUL','SurvivalTime'],axis=1)\ny = machineData['RUL']\n\n# split the data\nx_train, x_test, y_train, y_test = train_test_split(x_selected,y, test_size=0.2, random_state=42)\n\n\n\nfit the model\n\n# Create a Random Forest Regressor model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model to the training data\nrf_model.fit(x_train, y_train)\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nPredictions\n\n# Make predictions on the testing data\ny_pred = rf_model.predict(x_test)\n\n\n\nValidations\n\n# Calculate the Mean Squared Error (MSE) and R-squared value of the predictions\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\n# Print the results\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"R-squared value: {r2}\")\n\nMean Squared Error: 83.94598232058489\nMean Absolute Error: 4.123105926662051\nR-squared value: 0.9874394628423803\n\n\n\n\ninterpret the model\n\n# shap to feature explanation\n\n# Fit the explainer to sample x_test/ full dataset x_selected\nexplainer = shap.Explainer(rf_model.predict, x_selected)\n\n# Calculates the SHAP values - It takes some time\nshap_values = explainer(x_selected)\nshap_values\n\nExact explainer: 19806it [3:41:26,  1.49it/s]                                                                          \n\n\n.values =\narray([[ 3.73907837e+00,  1.32506122e+00, -1.17333856e-01, ...,\n        -2.73252130e+00, -2.40410608e+00, -7.34367051e+00],\n       [ 3.73907837e+00,  1.32506122e+00, -1.17333856e-01, ...,\n        -2.73252130e+00, -2.40410608e+00, -7.34367051e+00],\n       [ 3.70568670e+00,  1.32506122e+00, -1.17543856e-01, ...,\n        -2.73768463e+00, -2.42076108e+00, -7.36847884e+00],\n       ...,\n       [-7.33903858e+01,  1.89993439e+00, -1.93152320e-02, ...,\n         2.65801312e-01,  2.58013244e-01, -4.20636435e-01],\n       [-7.33903858e+01,  1.89993439e+00, -1.93152320e-02, ...,\n         2.65801312e-01,  2.58013244e-01, -4.20636435e-01],\n       [-7.33903858e+01,  1.89993439e+00, -1.93152320e-02, ...,\n         2.65801312e-01,  2.58013244e-01, -4.20636435e-01]])\n\n.base_values =\narray([115.95513874, 115.95513874, 115.95513874, ..., 115.95513874,\n       115.95513874, 115.95513874])\n\n.data =\narray([[  0,   0,   0, ...,  16,  16,   3],\n       [  0,   0,   0, ...,  16,  16,   3],\n       [  0,   0,   0, ...,  16,  16,   3],\n       ...,\n       [136,   0,   0, ...,   0,   0,   0],\n       [136,   0,   0, ...,   0,   0,   0],\n       [136,   0,   0, ...,   0,   0,   0]], dtype=int64)\n\n\n\n# bee swarm plot to understand feature importance on dataset\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\nModel - Pycaret\nModel selection, find best model for the task\n\n# without any customization\nreg_normal_setup = setup(data=machineData,\n                        target = 'RUL',\n                        ignore_features=['date','failure','SurvivalTime'],\n                        )\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n937\n\n\n1\nTarget\nRUL\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(19805, 14)\n\n\n4\nTransformed data shape\n(19805, 11)\n\n\n5\nTransformed train set shape\n(13863, 11)\n\n\n6\nTransformed test set shape\n(5942, 11)\n\n\n7\nIgnore features\n3\n\n\n8\nNumeric features\n10\n\n\n9\nPreprocess\nTrue\n\n\n10\nImputation type\nsimple\n\n\n11\nNumeric imputation\nmean\n\n\n12\nCategorical imputation\nmode\n\n\n13\nFold Generator\nKFold\n\n\n14\nFold Number\n10\n\n\n15\nCPU Jobs\n-1\n\n\n16\nUse GPU\nFalse\n\n\n17\nLog Experiment\nFalse\n\n\n18\nExperiment Name\nreg-default-name\n\n\n19\nUSI\n3334\n\n\n\n\n\n\n# best_model = compare_models(sort='MAE')\n# TT - training time (sec) , MAPE = mean absolute percentage error\nbest_model = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\net\nExtra Trees Regressor\n4.7575\n101.3286\n10.0449\n0.9853\n0.2840\n0.1464\n1.1630\n\n\nrf\nRandom Forest Regressor\n5.1086\n108.5043\n10.3796\n0.9843\n0.3124\n0.1894\n1.5270\n\n\nxgboost\nExtreme Gradient Boosting\n7.7985\n137.9218\n11.7297\n0.9800\n0.3801\n0.3016\n0.7830\n\n\ndt\nDecision Tree Regressor\n4.9681\n162.1656\n12.6559\n0.9765\n0.3251\n0.1556\n0.5590\n\n\ncatboost\nCatBoost Regressor\n11.4940\n238.4826\n15.4325\n0.9654\n0.4614\n0.4280\n2.2080\n\n\nlightgbm\nLight Gradient Boosting Machine\n13.3388\n332.2891\n18.2104\n0.9519\n0.5164\n0.5319\n0.7670\n\n\ngbr\nGradient Boosting Regressor\n32.8338\n1855.2679\n43.0527\n0.7312\n0.7725\n1.2410\n0.7690\n\n\nada\nAdaBoost Regressor\n54.0198\n4126.8465\n64.2232\n0.4018\n1.0351\n2.2930\n0.6490\n\n\nridge\nRidge Regression\n65.0578\n6202.9049\n78.7452\n0.1009\n1.1209\n2.6862\n0.6190\n\n\nllar\nLasso Least Angle Regression\n65.0600\n6202.6741\n78.7438\n0.1009\n1.1209\n2.6867\n0.5230\n\n\nen\nElastic Net\n65.0593\n6202.8387\n78.7448\n0.1009\n1.1209\n2.6867\n0.5510\n\n\nlasso\nLasso Regression\n65.0600\n6202.6741\n78.7438\n0.1009\n1.1209\n2.6867\n0.5770\n\n\nlr\nLinear Regression\n65.0578\n6202.9049\n78.7452\n0.1009\n1.1209\n2.6862\n1.3680\n\n\nlar\nLeast Angle Regression\n65.0593\n6203.5352\n78.7492\n0.1008\n1.1210\n2.6859\n0.5660\n\n\nbr\nBayesian Ridge\n65.0606\n6203.2754\n78.7475\n0.1008\n1.1208\n2.6876\n0.5460\n\n\nomp\nOrthogonal Matching Pursuit\n69.4202\n6870.2529\n82.8726\n0.0042\n1.1810\n3.0466\n0.5390\n\n\ndummy\nDummy Regressor\n69.6857\n6905.0759\n83.0870\n-0.0009\n1.1837\n3.0479\n0.5870\n\n\nknn\nK Neighbors Regressor\n69.1033\n7199.8013\n84.8340\n-0.0435\n1.1727\n2.9935\n0.5890\n\n\nhuber\nHuber Regressor\n74.5711\n8757.1132\n93.5631\n-0.2694\n1.2011\n2.5065\n0.6100\n\n\npar\nPassive Aggressive Regressor\n107.5586\n19862.1006\n129.4187\n-1.8903\n1.4562\n3.5808\n0.5380\n\n\n\n\n\n\n\n\n\nprint(best_model)\n\nExtraTreesRegressor(n_jobs=-1, random_state=937)\n\n\n\nHyperparameter tuning\nFind best combination of hyperparameters from Hyperparameter space. - Every combination in HP space is model - To search effective combination of HP - two approaches - GridSeachCV and RandomSeachCV where CV = cross-validation - with idea of hyperparameter tuning, split the data into trainig, testing and validation to prevent ‘Data Leakage’\n\n# tune the hyper parameters of the best model - case when : tree model\n\"\"\"\nimportant : max_features, min_samples_leaf,n_estimators\n\"\"\"\nparams = {\n    'n_estimators': [100],           #number of trees\n    'criterion': ['absolute_error'], #['squared_error','absolute_error','friedman_mse','poisson']\n    'max_depth': [None],             # if none nodes are expanded until all leaves are pure\n    'min_samples_split': [2],        # min samples required to split internal node [int or float or default=2]\n    'min_samples_leaf' : [1],        # a split is considered if both branches has min_samples_leaf [int or float or default=1]\n    # min_weight_fraction_leaf : 0.0\n    'max_features':[1.0],            # max features to consider at each split [int or float or none or 'sqrt' or 'log'] default = 1.0 =&gt; max_features=n_features\n    'max_leaf_nodes': [None],        # int = first best nodes or infinite nodes if none\n    'min_impurity_decrease' : [0.0], # node split is considered if decrease of impurity due to this split &lt; min_impurity_decrease\n    'bootstrap':[False],             # True : use bootstrap samples to build the tree , False: Use whole dataset to build each tree\n    'oob_score' : [False],           # out of bag samples to estimate generalization score , only when bootstrap= True\n    # 'n_jobs'=1,                    # number of jobs to run in parallel\n    'random_state' : [None],         # for bootstrapping, draw of split of max_Features\n    'verbose' : [True], \n    # 'warm_start' : False           # True : reuse the solution of previous call to add estimator to ensemble\n    'ccp_alpha' : [0.0],             # minimal cost complexity pruning ,0.0 = no pruning , largest cost complexity &lt; ccp_alpha is choosen\n    'max_samples' : [None]           #max samples to draw to train estimator (if bootstrapping = True)   \n}\n\n\nextra_tree_tuned_model = tune_model(best_model,\n                         fold = 2,\n                         n_iter=10,\n                         custom_grid = params,\n                         optimize='MAE',\n                         search_library='scikit-learn',\n                         search_algorithm='random',\n                         choose_better = True, # True : return only better performing model b/w original estimator or tuned model\n                         return_tuner= False, # if True: returns tuner object , False: returns estimator object very important difference\n                         return_train_score=True\n                        )\n\n\n\n\n\n\n\n\n\n \n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nSplit\nFold\n \n \n \n \n \n \n\n\n\n\nCV-Train\n0\n0.0001\n0.0001\n0.0085\n1.0000\n0.0060\n0.0001\n\n\n1\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nCV-Val\n0\n7.8070\n244.7006\n15.6429\n0.9641\n0.3639\n0.2580\n\n\n1\n7.8475\n250.6826\n15.8330\n0.9641\n0.3799\n0.2662\n\n\nCV-Train\nMean\n0.0001\n0.0000\n0.0042\n1.0000\n0.0030\n0.0000\n\n\nStd\n0.0001\n0.0000\n0.0042\n0.0000\n0.0030\n0.0000\n\n\nCV-Val\nMean\n7.8273\n247.6916\n15.7379\n0.9641\n0.3719\n0.2621\n\n\nStd\n0.0203\n2.9910\n0.0950\n0.0000\n0.0080\n0.0041\n\n\nTrain\nnan\n0.0001\n0.0001\n0.0085\n1.0000\n0.0043\n0.0000\n\n\n\n\n\n\n\n\nFitting 2 folds for each of 1 candidates, totalling 2 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\nprint(extra_tree_tuned_model)\n\nExtraTreesRegressor(n_jobs=-1, random_state=937)\n\n\n\n# avaiable plots in pycaret\navailable_plots = [\n    'pipeline', # schematic drawing of the preprocessing pipeline\n    'residuals_interactive', # interactive residual plots\n    'residuals', \n    'error',\n    'cooks', # cooks distance plot ?\n    'rfe',\n    'learning',\n    'vc',\n    'manifold',\n    'feature',\n    'feature_all',\n    'parameter',\n    'tree' # Decision tree\n]\n\n\nplot_model(extra_tree_tuned_model,plot='error')\n\n\n\n\n\n\n\n\nplot_model(best_model,plot=\"error\")\n\n\nplot_model(extra_tree_tuned_model,plot='feature')\n\n\n\n\n\n\n\n\n# Interpret the model - dependent on SHAP\n# 40 minutes\ninterpret_model(extra_tree_tuned_model,plot='reason',observation=32)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# save model\n# save_model(best_model,'Extra_Trees_regressor_model')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\Lalitha\\AppData\\Local\\Temp\\joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(include=['device', 'metric1', 'metric2',\n                                              'metric3', 'metric4', 'metric5',\n                                              'metric6', 'metric7', 'metric8',\n                                              'metric9'],\n                                     transformer=SimpleImputer())),\n                 ('categorical_imputer',\n                  TransformerWrapper(include=[],\n                                     transformer=SimpleImputer(strategy='most_frequent'))),\n                 ('trained_model',\n                  ExtraTreesRegressor(n_jobs=-1, random_state=937))]),\n 'Extra_Trees_regressor_model.pkl')\n\n\n\n#dashboard(model) - uses explainerDashboard library \ndashboard_kwargs = {\n    'shap_interaction':False,\n    'show_metrics':['root_mean_squared_error', 'mean_absolute_error','R-squared']\n}\ndashboard(best_model,display_format='inline',dashboard_kwargs=dashboard_kwargs)\n\nChanging class type to RandomForestRegressionExplainer...\nGenerating self.shap_explainer = shap.TreeExplainer(model)\nBuilding ExplainerDashboard..\nWARNING: the number of idxs (=5942) &gt; max_idxs_in_dropdown(=1000). However with your installed version of dash(2.9.3) dropdown search may not work smoothly. You can downgrade to `pip install dash==2.6.2` which should work better for now...\nGenerating layout...\nCalculating shap values..."
  },
  {
    "objectID": "Library Notebooks/visualize.html",
    "href": "Library Notebooks/visualize.html",
    "title": "Data Analysis and Visualization",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nsource\n\nplot\n\n plot (feature_type:str, feature:str, dataset:pandas.core.frame.DataFrame,\n       boxplot:bool=True, histogram:bool=True, barchart:bool=True)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_type\nstr\n\nwhether feature is numerical or categorical\n\n\nfeature\nstr\n\nname of feature\n\n\ndataset\nDataFrame\n\ndataframe of dataset\n\n\nboxplot\nbool\nTrue\nwhether draw box plot or not\n\n\nhistogram\nbool\nTrue\nwhether draw histogram or not\n\n\nbarchart\nbool\nTrue\nwhether to draw bar chart or not"
  },
  {
    "objectID": "Library Notebooks/model.html",
    "href": "Library Notebooks/model.html",
    "title": "Model",
    "section": "",
    "text": "““” Because we are dealing with cross-sectional time-series, it is better not to take a random sample of all records. Doing so would put the records from one machine in all three sample data sets. To avoid this, we’ll randomly select IDs and place all of the records for each machine in either the training, testing, or validation data set. ““”\n\nsource\n\ntrain_test_split\n\n train_test_split (dataset_df:pandas.core.frame.DataFrame,\n                   machine_Unique_Identifer:str, failure:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndataset_df\nDataFrame\nPandas DataFrame object of dataset\n\n\nmachine_Unique_Identifer\nstr\na unique ID to identify machine\n\n\nfailure\nstr\nevent of failute, failure = 0 means not failed yet, failure=1 means equipment is failed\n\n\n\n\nsource\n\n\nbuild_test_train\n\n build_test_train (dataset_df:pandas.core.frame.DataFrame,\n                   test_part:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\ndataset_df\nDataFrame\nPandas DataFrame object of dataset\n\n\ntest_part\nfloat\nsplit criteria"
  },
  {
    "objectID": "Library Notebooks/datasets.html",
    "href": "Library Notebooks/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "source\n\nexplore_dataset\n\n explore_dataset (dataset_df:pandas.core.frame.DataFrame, NAN_action:str,\n                  duplicate_action:str, NAN_subset:list=None,\n                  duplicate_subset:list=None, inplace:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_df\nDataFrame\n\nDataFrame handler of the dataset\n\n\nNAN_action\nstr\n\naction to take on null values\n\n\nduplicate_action\nstr\n\naction to take on duplicate values\n\n\nNAN_subset\nlist\nNone\ndrop rows with null values in columns of NAN_subset\n\n\nduplicate_subset\nlist\nNone\ndrop rows with duplicate values in columns of dupliate_subset\n\n\ninplace\nbool\nTrue\nwhether ot not to make changes in original dataset\n\n\nReturns\nNone\n\nthere is nothin to return\n\n\n\n\nsource\n\n\nload_dataset\n\n load_dataset (filepath:str, mode:str)"
  },
  {
    "objectID": "Library Notebooks/featureeng.html",
    "href": "Library Notebooks/featureeng.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "source\n\nexplain_features\n\n explain_features (dataset_df:pandas.core.frame.DataFrame,\n                   machine_Unique_Identifer:str=None,\n                   machine_features:list=None, observation_date:str=None,\n                   survival_time:int=None, failure:int=None,\n                   sensor_values:list=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_df\nDataFrame\n\nPandas DataFrame object of dataset\n\n\nmachine_Unique_Identifer\nstr\nNone\na unique ID to identify machine\n\n\nmachine_features\nlist\nNone\na list of machine features such as vendor_name, machine_type,manufacturer ….\n\n\nobservation_date\nstr\nNone\ndate when observation is recorded\n\n\nsurvival_time\nint\nNone\nage of equipment till observation_date\n\n\nfailure\nint\nNone\nevent of failute . In most cases failure = 0 means not failed yet, failure=1 means equipment is failed\n\n\nsensor_values\nlist\nNone\nmeasured values of multiple sensors\n\n\n\n\nsource\n\n\nexpand_target_window\n\n expand_target_window (dataset_df:pandas.core.frame.DataFrame,\n                       target_window:int, observation_date:str,\n                       machine_Unique_Identifer:str, rul:str=None,\n                       survival_time:str=None, failure_date:str=None)\n\n\nsource\n\n\nremove_invalid_records\n\n remove_invalid_records (dataset_df:pandas.core.frame.DataFrame,\n                         machine_Unique_Identifer:str=None,\n                         observation_date:str=None, failure:str=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_df\nDataFrame\n\nPandas DataFrame object of dataset\n\n\nmachine_Unique_Identifer\nstr\nNone\na unique ID to identify machine\n\n\nobservation_date\nstr\nNone\ndate when observation is recorded\n\n\nfailure\nstr\nNone\nevent of failute . In most cases failure = 0 means not failed yet, failure=1 means equipment is failed\n\n\nReturns\nDataFrame\n\ndataframe after removing invalid records\n\n\n\n\nsource\n\n\ncalculate_rul\n\n calculate_rul (dataset_df:pandas.core.frame.DataFrame,\n                observation_date:str, machine_Unique_Identifer:str)\n\n\nsource\n\n\nto_numerical\n\n to_numerical (dataset_df:pandas.core.frame.DataFrame)\n\n\nsource\n\n\ncalculate_survival_time\n\n calculate_survival_time (dataset_df:pandas.core.frame.DataFrame,\n                          observation_date:str,\n                          machine_Unique_Identifer:str)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predictive Maintenance",
    "section": "",
    "text": "use the package by installing it using the given instructions On windows,mac or Linux...\npip install PredictiveMaintenance2"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Predictive Maintenance",
    "section": "",
    "text": "use the package by installing it using the given instructions On windows,mac or Linux...\npip install PredictiveMaintenance2"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Predictive Maintenance",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  }
]